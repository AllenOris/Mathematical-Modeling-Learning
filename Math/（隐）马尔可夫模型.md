> 最近稍稍深看一点点机器学习的东西，感觉自己的概率论水平真滴不够。从贝叶斯概率到各种
随机过程。需要大补特补。

papers: [Hidden Markov Models Fundamentals](http://cs229.stanford.edu/section/cs229-hmm.pdf), [An introduction to Markov Model](http://ai.stanford.edu/~pabbeel/depth_qual/Rabiner_Juang_hmms.pdf), 统计学习方法第二版

## 马尔可夫模型

核心就是两个假设：1. 每一个状态只与前一次的状态相关；2. 转移概率不变

两个基本问题：

1. 得到一个序列的概率

> 由于state都是暴露的，直接乘法法则就好了

2. 对于观测值，寻找最大可能的参数

> 这个序列里，所有i->x中i->j的转换频率为得到的$A_{ij}$最大似然估计。（最大似然估计的解由拉格朗日乘数法求得）

## 隐马尔可夫模型

三个基本问题：

1. 得到某个观测值序列的概率？

* 直接计算法：对于得到这一个观测序列的每一个可能的状态序列，累加它们输出这个观测序列的概率\*它们本身出现的概率。【朴素的方法复杂度很高， O(|S|^T )】 

* forward procedure. -- 动态规划。

前向概率：给定马尔可夫模型，定义到时刻t部分观测序列为o1...ot，且状态为q\_i的概率为前向概率，$\alpha_t(i)$

前向算法根据初值，网格状递推的计算。【每一次迭代，下一轮各节点的值是通过前一轮每个节点转变至它的概率之和，然后就一直迭代就可以了。迭代过程根据观测序列在这个时刻的t，进行计算】

* backward procedure.  从后往前的前向概率

借助前向概率和后向概率，可以得到一些有用的期望值，会简化下两个问题的思考。分别是在观测O下，某个状态i的期望值；在观测O下，由状态i转移的期望值【不考虑最后一轮即可】；在观测O下，由状态i转移到j的期望值。

2. 得到观测值序列的最可能状态序列？

* 近似算法：取去每个时刻出现可能最大的那个state，作为这个时刻的state。【没有考虑状态之间的关联，有的时候会产生不可能相邻序列，不过还是有用】

* [维特比算法](https://zh.wikipedia.org/wiki/%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95)，“穿过格式结构的最长路径”。和前向算法一致，只不过对于每一轮中的某个state，只取前一轮到它概率最大的那个概率作为它的概率进行迭代计算，并记录状态个数个state sequence，最后在backtrack下。【因为状态序列是确定的，只选取一个最大的。而前向概率计算的是观测值序列，所以迭代过程在累加】

> 仍然使用Markov性简化。从头开始，计算到下一个时间点各状态分别的最大概率【概率的计算依存于上一步最大概率的计算、状态转移矩阵和输出概率矩阵】，【并维护走i步到状态z的最大概率序列【即有状态数量个序列】】，最后找到达到期望步数k后，最大的概率序列。【对于输出的考虑，是在迭代时计算下一步最可能的概率的过程中计算的，最后backtracking不再需要考虑输出】

3. 如何学习两个参数矩阵？Baum-Welch算法（EM算法）

首先说明，可能需要多次计算，因为要优化的Q函数是non-convex的。

E步：说明算法迭代优化的性质

M步：说明怎样极大化模型参数

> 猜（E-step）,反思（M-step）,重复

细节不说了。直观下，计算的根据是：转移矩阵（i->j）的预测值是（i->j转移期望/i的转移期望）；观测矩阵的预测值（i输出k）是（i输出k的期望/i出现的期望）；初始转移概率（$\pi$）是（在时刻t=1时，处于各个状态的概率）

> 看了知乎这个解释，感觉有些懂啦。但没有很明白是迭代优化是如何可行的（为什么迭代后会越来越好）。[解释1](https://www.zhihu.com/question/27976634);[解释二](https://www.jianshu.com/p/1121509ac1dc); 


### application

> 两篇论文，今天看不下去了TwT【wuwuwuwu...】


