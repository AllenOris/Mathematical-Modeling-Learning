> 内容：信息熵、条件熵、互信息、相对熵
> 
> 信息论与概率论紧密结合.
> 
> 关于[贝叶斯分布](http://www.xuyankun.cn/2017/05/13/bayes/)的内容不怎么能看懂，还需要理解下.

## 信息熵

对于一个问题，它具有许多的可能性，这些可能性构成了这个问题的不确定性（熵）.一条完全确定一个问题的信息，它的信息量取决于对这个问题的可能性空间最小的猜测次数.【最快的猜测方式及“二分”的猜测】.

此外，对于对于一个问题，它的各种可能情况，概率或许不同.可以想象，若所有可能情况概率相同，不确定性便越强（最大熵）.因而可以得出信息熵的计算形式.

$$H =- \sum {p_ilogp_i}$$

> [关于信息熵最大值的证明](https://blog.csdn.net/feixi7358/article/details/83861858)
> 思路大概是，考虑两种可能性时，公式的极值可能；再不断合并.

可以用信息熵量化语言.[汉子信息熵和语言模型的复杂度](http://dsd.future-lab.cn/members/2015nlp/readings/%E6%B1%89%E8%AF%AD%E4%BF%A1%E6%81%AF%E7%86%B5%E5%92%8C%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%A4%8D%E6%9D%82%E5%BA%A6.pdf)考虑上下文的相关性，汉字是用的频率，中文的每个汉字信息熵只有5bit左右.中文汉字的冗余度很大，但是所有语言中冗余度较小的.

在对信息作出针对的处理时，不应试图引入人为的假设.

## 条件熵

两个问题，它们之间有相互联系，假设知道其中一个问题的实际情况，将减少另一个问题的搜索空间.即引入联合分布概率.

$$H =- \sum {p_{X|Y}logp_{X|Y}}$$

【在已知Y事件下，X的不确定度】

## 互信息

互信息用于衡量两个随机事件的相关性.本质上描述了联合分布，与边缘分布之积的差异程度【可以转换为相对熵的形式】.

$I(X; Y) = H(X) - H(X|Y)$：Y的确定，对于X事件不确定度的贡献的信息量.

## [相对熵（交叉熵，KL散度）](https://zhuanlan.zhihu.com/p/36192699)

用于衡量两个取值为正数的函数的相关性，可以对两个分布之间的差异程度进行量化.【相对熵是不对称的】.如果我们使⽤了不同于真实分布的概率分布，那么我们⼀定会损失编码效率，并且在传输时增加的平均额外信息量⾄少等于两个分布之间的Kullback-Leibler散度.还需要实战、应用理解一下.

## 语言模型复杂度

语言模型复杂度从条件熵和相对熵的概念除法，用于直接衡量语言模型的好坏.它被定义为，在给定上下文的条件下，句子中每个位置平均可以选择的单词数量.一个模型的复杂度越小，每个位置的单词越确定，模型越好.

## 微分熵

利用微分的手段，将连续变量x切分成宽度为$\delta$的箱子，可以得到一个表达式= −∑\_{i}
p(xi)∆ln(p(xi))−ln ∆，取左边，换为积分号为微分熵；熵的离散形式是与连续形式相差$ln\delta$，在趋向于0时，为无穷大，说明完全具体化一个连续变量是不可能的.

考虑最大化微分熵，设定三个关于概率的限制条件 => 最大化微分熵是高斯分布.



